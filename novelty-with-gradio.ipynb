{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13670243,"sourceType":"datasetVersion","datasetId":8692049},{"sourceId":13898819,"sourceType":"datasetVersion","datasetId":8855049},{"sourceId":13903291,"sourceType":"datasetVersion","datasetId":8857949}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":18323.451757,"end_time":"2025-11-02T08:39:19.786605","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-11-02T03:33:56.334848","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi\n!pip -q install --upgrade pip\n!pip -q install basicsr timm einops gdown imageio opencv-python albumentations matplotlib torchvision numpy==1.26.4 scipy==1.13.1\n\n# Fresh clone\n!rm -rf Restormer\n!git clone https://github.com/swz30/Restormer.git\n%cd Restormer","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-11-28T10:55:55.290849Z","iopub.execute_input":"2025-11-28T10:55:55.291126Z","iopub.status.idle":"2025-11-28T10:56:00.374284Z","shell.execute_reply.started":"2025-11-28T10:55:55.291104Z","shell.execute_reply":"2025-11-28T10:56:00.373556Z"},"papermill":{"duration":245.269466,"end_time":"2025-11-02T03:38:05.182881","exception":false,"start_time":"2025-11-02T03:33:59.913415","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Fri Nov 28 10:55:55 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   38C    P0             33W /  250W |    8337MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\nCloning into 'Restormer'...\nremote: Enumerating objects: 312, done.\u001b[K\nremote: Counting objects: 100% (115/115), done.\u001b[K\nremote: Compressing objects: 100% (43/43), done.\u001b[K\nremote: Total 312 (delta 74), reused 72 (delta 72), pack-reused 197 (from 2)\u001b[K\nReceiving objects: 100% (312/312), 1.55 MiB | 25.56 MiB/s, done.\nResolving deltas: 100% (131/131), done.\n/kaggle/working/Restormer/Restormer/Restormer/Restormer/Restormer/Restormer/Restormer/Restormer/Restormer/Restormer\n","output_type":"stream"}],"execution_count":82},{"cell_type":"code","source":"import numpy as np\nimport scipy\n\nprint(\"NumPy version:\", np.__version__)\nprint(\"SciPy version:\", scipy.__version__)","metadata":{"papermill":{"duration":0.018868,"end_time":"2025-11-02T03:38:05.206524","exception":false,"start_time":"2025-11-02T03:38:05.187656","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T10:56:05.871975Z","iopub.execute_input":"2025-11-28T10:56:05.872267Z","iopub.status.idle":"2025-11-28T10:56:05.877109Z","shell.execute_reply.started":"2025-11-28T10:56:05.872240Z","shell.execute_reply":"2025-11-28T10:56:05.876324Z"}},"outputs":[{"name":"stdout","text":"NumPy version: 1.26.4\nSciPy version: 1.13.1\n","output_type":"stream"}],"execution_count":83},{"cell_type":"code","source":"# ---- TorchVision shim for legacy imports (functional_tensor) ----\nimport sys, types\nimport torch, torchvision\nprint(\"Torch:\", torch.__version__, \"| TorchVision:\", torchvision.__version__)\n\nfrom torchvision.transforms import functional as F\nft_mod = types.ModuleType(\"torchvision.transforms.functional_tensor\")\nfor k, v in F.__dict__.items():\n    setattr(ft_mod, k, v)\nsys.modules[\"torchvision.transforms.functional_tensor\"] = ft_mod\nprint(\"Shim installed: torchvision.transforms.functional_tensor ‚Üí .functional ‚úÖ\")","metadata":{"execution":{"iopub.status.busy":"2025-11-28T10:56:07.777812Z","iopub.execute_input":"2025-11-28T10:56:07.778420Z","iopub.status.idle":"2025-11-28T10:56:07.783736Z","shell.execute_reply.started":"2025-11-28T10:56:07.778387Z","shell.execute_reply":"2025-11-28T10:56:07.782904Z"},"papermill":{"duration":7.980004,"end_time":"2025-11-02T03:38:13.190689","exception":false,"start_time":"2025-11-02T03:38:05.210685","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Torch: 2.6.0+cu124 | TorchVision: 0.21.0+cu124\nShim installed: torchvision.transforms.functional_tensor ‚Üí .functional ‚úÖ\n","output_type":"stream"}],"execution_count":84},{"cell_type":"code","source":"# ‚úÖ Run this as a shell cell (notice the ! at the start of each command)\nCKPT_URL=\"https://github.com/swz30/Restormer/releases/download/v1.0/deraining.pth\"\n\n!mkdir -p ./checkpoints\n!rm -f ./checkpoints/pretrained_task.pth\n!curl -L \"$CKPT_URL\" -o ./checkpoints/pretrained_task.pth\n!ls -lh ./checkpoints","metadata":{"execution":{"iopub.status.busy":"2025-11-28T10:56:09.989959Z","iopub.execute_input":"2025-11-28T10:56:09.990540Z","iopub.status.idle":"2025-11-28T10:56:11.478817Z","shell.execute_reply.started":"2025-11-28T10:56:09.990517Z","shell.execute_reply":"2025-11-28T10:56:11.477836Z"},"papermill":{"duration":1.769455,"end_time":"2025-11-02T03:38:14.964827","exception":false,"start_time":"2025-11-02T03:38:13.195372","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100 99.8M  100 99.8M    0     0   113M      0 --:--:-- --:--:-- --:--:--  154M\ntotal 100M\n-rw-r--r-- 1 root root 100M Nov 28 10:56 pretrained_task.pth\n","output_type":"stream"}],"execution_count":85},{"cell_type":"code","source":"import os, torch\nCKPT_PATH = \"./checkpoints/pretrained_task.pth\"\nprint(\"File size (MB):\", os.path.getsize(CKPT_PATH)/1e6)\n# Should be roughly > 50 MB\nassert os.path.getsize(CKPT_PATH) > 10_000_000, \"Checkpoint looks incomplete!\"\n\n# Quick load test\ntry:\n    _ = torch.load(CKPT_PATH, map_location=\"cpu\")\n    print(\"‚úÖ torch.load works fine ‚Äî checkpoint OK!\")\nexcept Exception as e:\n    print(\"‚ùå Problem loading checkpoint:\", e)","metadata":{"execution":{"iopub.status.busy":"2025-11-28T10:56:12.547760Z","iopub.execute_input":"2025-11-28T10:56:12.548583Z","iopub.status.idle":"2025-11-28T10:56:12.659058Z","shell.execute_reply.started":"2025-11-28T10:56:12.548550Z","shell.execute_reply":"2025-11-28T10:56:12.658316Z"},"papermill":{"duration":0.145171,"end_time":"2025-11-02T03:38:15.114824","exception":false,"start_time":"2025-11-02T03:38:14.969653","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"File size (MB): 104.700429\n‚úÖ torch.load works fine ‚Äî checkpoint OK!\n","output_type":"stream"}],"execution_count":86},{"cell_type":"code","source":"# ==== RECOVERY: make sure Restormer is present and importable ====\nimport os, sys, subprocess, shutil, glob, importlib.util\nfrom pathlib import Path\n\nREPO_URL  = \"https://github.com/swz30/Restormer.git\"\nREPO_DIR  = Path(\"/kaggle/working/Restormer\")   # fixed absolute path\n\n# 1) Fresh clone if missing or empty\ndef is_dir_empty(p: Path):\n    return (not p.exists()) or (p.exists() and len(list(p.rglob(\"*\"))) == 0)\n\nif is_dir_empty(REPO_DIR):\n    if REPO_DIR.exists():\n        shutil.rmtree(REPO_DIR, ignore_errors=True)\n    print(\"[i] Cloning Restormer repo...\")\n    subprocess.check_call([\"git\", \"clone\", \"--depth\", \"1\", REPO_URL, str(REPO_DIR)])\nelse:\n    print(\"[i] Restormer repo already present at\", REPO_DIR)\n\n# 2) Put repo on sys.path so 'basicsr' (inside repo) is importable\nif str(REPO_DIR) not in sys.path:\n    sys.path.insert(0, str(REPO_DIR))\nif str(REPO_DIR.parent) not in sys.path:\n    sys.path.insert(0, str(REPO_DIR.parent))\n\n# 3) Try canonical import; if it fails, import by file path\nRestormer = None\ntry:\n    from basicsr.models.archs.restormer_arch import Restormer  # type: ignore\n    print(\"[‚úì] Imported Restormer from basicsr.models.archs.restormer_arch\")\nexcept Exception as e:\n    print(\"[!] Canonical import failed:\", e)\n    cand = list(REPO_DIR.rglob(\"restormer_arch.py\"))\n    assert cand, \"restormer_arch.py not found under repo\"\n    mod_path = cand[0]\n    spec = importlib.util.spec_from_file_location(\"restormer_local\", str(mod_path))\n    mod = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(mod)  # type: ignore\n    Restormer = getattr(mod, \"Restormer\")\n    print(f\"[‚úì] Imported Restormer from file: {mod_path}\")","metadata":{"execution":{"iopub.status.busy":"2025-11-28T10:56:15.793995Z","iopub.execute_input":"2025-11-28T10:56:15.794270Z","iopub.status.idle":"2025-11-28T10:56:15.844207Z","shell.execute_reply.started":"2025-11-28T10:56:15.794250Z","shell.execute_reply":"2025-11-28T10:56:15.843556Z"},"papermill":{"duration":0.642685,"end_time":"2025-11-02T03:38:15.762404","exception":false,"start_time":"2025-11-02T03:38:15.119719","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"[i] Restormer repo already present at /kaggle/working/Restormer\n[!] Canonical import failed: No module named 'basicsr.models.archs'\n[‚úì] Imported Restormer from file: /kaggle/working/Restormer/Restormer/Restormer/Restormer/Restormer/Restormer/Restormer/Restormer/Restormer/Restormer/basicsr/models/archs/restormer_arch.py\n","output_type":"stream"}],"execution_count":87},{"cell_type":"code","source":"# === ONE-CELL SAFE LOADER FOR RESTORMER + CHECKPOINT ===\nimport sys, types, importlib.util, re\nfrom pathlib import Path\nfrom collections import OrderedDict\nimport torch\n\n# ---- Shim for legacy torchvision import used by some basicsr code ----\ntry:\n    from torchvision.transforms import functional as F\n    ft_mod = types.ModuleType(\"torchvision.transforms.functional_tensor\")\n    for k, v in F.__dict__.items():\n        setattr(ft_mod, k, v)\n    sys.modules[\"torchvision.transforms.functional_tensor\"] = ft_mod\n    print(\"Shim ok: torchvision.transforms.functional_tensor -> .functional\")\nexcept Exception as e:\n    print(\"Shim skipped:\", e)\n\n# ---- Robust import of Restormer (canonical or file-path fallback) ----\nrepo_root = \"/kaggle/working/Restormer\"\nif repo_root not in sys.path:\n    sys.path.append(repo_root)\n\nRestormer = None\ntry:\n    from basicsr.models.archs.restormer_arch import Restormer  # type: ignore\n    print(\"Imported Restormer from basicsr.models.archs.restormer_arch\")\nexcept Exception as e:\n    print(\"Canonical import failed:\", e)\n    candidates = list(Path(repo_root).rglob(\"*restormer*.py\"))\n    assert candidates, \"Restormer source not found under repo_root\"\n    p = str(candidates[0])\n    spec = importlib.util.spec_from_file_location(\"restormer_local\", p)\n    mod = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(mod)  # type: ignore\n    Restormer = getattr(mod, \"Restormer\")\n    print(\"Imported Restormer from file:\", p)\n\n# ---- Load checkpoint and pick the correct inner state dict ----\nckpt_path = \"./checkpoints/pretrained_task.pth\"\nstate = torch.load(ckpt_path, map_location=\"cpu\")\n\nif isinstance(state, dict):\n    if isinstance(state.get(\"params_ema\"), dict):\n        sd = state[\"params_ema\"]; print(\"Using checkpoint['params_ema']\")\n    elif isinstance(state.get(\"params\"), dict):\n        sd = state[\"params\"];     print(\"Using checkpoint['params']\")\n    elif isinstance(state.get(\"state_dict\"), dict):\n        sd = state[\"state_dict\"]; print(\"Using checkpoint['state_dict']\")\n    elif isinstance(state.get(\"model\"), dict):\n        sd = state[\"model\"];      print(\"Using checkpoint['model']\")\n    else:\n        sd = state;               print(\"Using checkpoint as-is (flat dict)\")\nelse:\n    sd = state;                   print(\"Using checkpoint as-is (non-dict)\")\n\n# strip DDP prefix if present\nif any(k.startswith(\"module.\") for k in sd.keys()):\n    sd = OrderedDict((re.sub(r\"^module\\.\", \"\", k), v) for k, v in sd.items())\n    print(\"Stripped 'module.' prefixes\")\n\ndef build_model(layernorm_type: str):\n    return Restormer(\n        inp_channels=3, out_channels=3,\n        dim=48,\n        num_blocks=[4,6,6,8],\n        num_refinement_blocks=4,\n        heads=[1,2,4,8],\n        ffn_expansion_factor=2.66,\n        bias=False,\n        LayerNorm_type=layernorm_type,  # 'WithBias' or 'BiasFree'\n        dual_pixel_task=False\n    )\n\ndef try_load(layernorm_type: str):\n    m = build_model(layernorm_type)\n    missing, unexpected = m.load_state_dict(sd, strict=False)\n    print(f\"[{layernorm_type}] missing: {len(missing)} | unexpected: {len(unexpected)}\")\n    if missing:   print(\"  sample missing:\", missing[:5])\n    if unexpected:print(\"  sample unexpected:\", unexpected[:5])\n    return m, missing, unexpected\n\n# Try BiasFree first, then WithBias\nmodel, missing, unexpected = try_load(\"BiasFree\")\nif missing or unexpected:\n    print(\"Retrying WithBias ‚Ä¶\")\n    model, missing, unexpected = try_load(\"WithBias\")\n\nprint(\"\\nFinal -> Loaded with strict=False\")\nprint(\"missing:\", len(missing), \"unexpected:\", len(unexpected))","metadata":{"execution":{"iopub.status.busy":"2025-11-28T10:56:20.369990Z","iopub.execute_input":"2025-11-28T10:56:20.370710Z","iopub.status.idle":"2025-11-28T10:56:20.992658Z","shell.execute_reply.started":"2025-11-28T10:56:20.370685Z","shell.execute_reply":"2025-11-28T10:56:20.991914Z"},"papermill":{"duration":0.712708,"end_time":"2025-11-02T03:38:16.480337","exception":false,"start_time":"2025-11-02T03:38:15.767629","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Shim ok: torchvision.transforms.functional_tensor -> .functional\nCanonical import failed: No module named 'basicsr.models.archs'\nImported Restormer from file: /kaggle/working/Restormer/Restormer/Restormer/Restormer/Restormer/Restormer/Restormer/Restormer/Restormer/Restormer/basicsr/models/archs/restormer_arch.py\nUsing checkpoint['params']\n[BiasFree] missing: 0 | unexpected: 88\n  sample unexpected: ['encoder_level1.0.norm1.body.bias', 'encoder_level1.0.norm2.body.bias', 'encoder_level1.1.norm1.body.bias', 'encoder_level1.1.norm2.body.bias', 'encoder_level1.2.norm1.body.bias']\nRetrying WithBias ‚Ä¶\n[WithBias] missing: 0 | unexpected: 0\n\nFinal -> Loaded with strict=False\nmissing: 0 unexpected: 0\n","output_type":"stream"}],"execution_count":88},{"cell_type":"code","source":"# freeze everything\nfor p in model.parameters(): p.requires_grad = False\n\n# unfreeze late/refinement + output head (adjust names if needed)\nto_unfreeze = []\nfor n, p in model.named_parameters():\n    if any(k in n.lower() for k in [\"refinement\", \"reconstruct\", \"reconstruction\", \"conv_out\", \"tail\"]):\n        p.requires_grad = True\n        to_unfreeze.append(n)\n\n# fallback: if nothing matched, unfreeze last ~10 params\nif not to_unfreeze:\n    for n,p in list(model.named_parameters())[-10:]:\n        p.requires_grad = True\n        to_unfreeze.append(n)\n\nprint(\"Unfreezing:\", *to_unfreeze[:8], \"...\", sep=\"\\n\")\n\nimport torch\ntrainable = [p for p in model.parameters() if p.requires_grad]\nopt_G = torch.optim.Adam(trainable, lr=1e-5, betas=(0.9,0.999))","metadata":{"execution":{"iopub.status.busy":"2025-11-28T10:56:22.750908Z","iopub.execute_input":"2025-11-28T10:56:22.751677Z","iopub.status.idle":"2025-11-28T10:56:22.762203Z","shell.execute_reply.started":"2025-11-28T10:56:22.751650Z","shell.execute_reply":"2025-11-28T10:56:22.761593Z"},"papermill":{"duration":0.017228,"end_time":"2025-11-02T03:38:16.502739","exception":false,"start_time":"2025-11-02T03:38:16.485511","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Unfreezing:\nrefinement.0.norm1.body.weight\nrefinement.0.norm1.body.bias\nrefinement.0.attn.temperature\nrefinement.0.attn.qkv.weight\nrefinement.0.attn.qkv_dwconv.weight\nrefinement.0.attn.project_out.weight\nrefinement.0.norm2.body.weight\nrefinement.0.norm2.body.bias\n...\n","output_type":"stream"}],"execution_count":89},{"cell_type":"code","source":"import torch.nn as nn, torchvision\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = model.to(device).train()\n\n# perceptual VGG features\nvgg = torchvision.models.vgg19(weights=torchvision.models.VGG19_Weights.IMAGENET1K_V1).features[:16].eval().to(device)\nfor p in vgg.parameters(): p.requires_grad = False\nl1, mse = nn.L1Loss(), nn.MSELoss()\n\n# tiny PatchGAN\nclass PatchD(nn.Module):\n    def __init__(self, in_ch=3, base=64):\n        super().__init__()\n        def blk(ic, oc, norm=True):\n            m=[nn.Conv2d(ic, oc, 4, 2, 1)]\n            if norm: m+=[nn.InstanceNorm2d(oc, affine=True)]\n            m+=[nn.LeakyReLU(0.2, inplace=True)]\n            return nn.Sequential(*m)\n        self.net = nn.Sequential(\n            blk(in_ch, base, norm=False),\n            blk(base, base*2),\n            blk(base*2, base*4),\n            nn.Conv2d(base*4, 1, 3, 1, 1)\n        )\n    def forward(self, x): return self.net(x)\n\ndisc = PatchD().to(device).train()\nopt_D = torch.optim.Adam(disc.parameters(), lr=2e-4, betas=(0.5,0.999))\n\ndef adv_loss(pred, is_real): \n    tgt = torch.ones_like(pred) if is_real else torch.zeros_like(pred)\n    return mse(pred, tgt)\n\nL_ADV, L_PERC, L_ID = 1.0, 0.1, 0.1","metadata":{"execution":{"iopub.status.busy":"2025-11-28T10:56:25.108527Z","iopub.execute_input":"2025-11-28T10:56:25.108810Z","iopub.status.idle":"2025-11-28T10:56:26.756421Z","shell.execute_reply.started":"2025-11-28T10:56:25.108791Z","shell.execute_reply":"2025-11-28T10:56:26.755631Z"},"papermill":{"duration":11.627285,"end_time":"2025-11-02T03:38:28.134797","exception":false,"start_time":"2025-11-02T03:38:16.507512","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":90},{"cell_type":"code","source":"# ==== CELL 10 (MODIFIED): Single-Domain Data Loaders ====\nimport os, glob, random, cv2, torch\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\n\nBASE_DIR = \"/kaggle/input/claude-synthesis-new\"\n\nDATA_ROOTS = {\n    \"clean\":     os.path.join(BASE_DIR, \"clean\"),\n    \"fog\":       os.path.join(BASE_DIR, \"fog\"),\n    \"rain\":      os.path.join(BASE_DIR, \"rain\"),\n    \"lowlight\":  os.path.join(BASE_DIR, \"lowlight\"),\n}\n\nDEG_DOMAINS = [d for d in DATA_ROOTS.keys() if d != \"clean\"]\nCLEAN_DIR   = DATA_ROOTS[\"clean\"]\n\ndef list_images(path):\n    exts = (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.bmp\")\n    files = []\n    for e in exts:\n        files += glob.glob(os.path.join(path, e))\n        files += glob.glob(os.path.join(path, \"**\", e), recursive=True)\n    if not files:\n        raise FileNotFoundError(f\"No images found in {path}\")\n    return sorted(files)\n\n# MODIFIED: Simpler augmentations for EDAR training\ntrain_tf = A.Compose([\n    A.LongestMaxSize(max_size=384),\n    A.PadIfNeeded(min_height=384, min_width=384, border_mode=cv2.BORDER_REFLECT_101),\n    A.RandomCrop(256, 256),\n    A.HorizontalFlip(p=0.5),\n    # REMOVED: RandomBrightnessContrast (let EDAR learn from clean data)\n])\n\nval_tf = A.Compose([\n    A.LongestMaxSize(max_size=512),\n    A.PadIfNeeded(min_height=512, min_width=512, border_mode=cv2.BORDER_REFLECT_101),\n])\n\ndef to_tensor(img):\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = np.ascontiguousarray(img).astype(np.float32)/255.0\n    return torch.from_numpy(np.transpose(img,(2,0,1)))\n\n# MODIFIED: Paired dataset (degraded -> clean pairs, not random)\nclass PairedDataset(Dataset):\n    \"\"\"Returns paired (degraded, clean) samples\"\"\"\n    def __init__(self, deg_files, cln_files, transform=None, paired=True):\n        self.deg_files = deg_files\n        self.cln_files = cln_files\n        self.tf = transform\n        self.paired = paired\n        \n        # For paired training, ensure same count\n        if paired:\n            min_len = min(len(deg_files), len(cln_files))\n            self.deg_files = deg_files[:min_len]\n            self.cln_files = cln_files[:min_len]\n    \n    def __len__(self): \n        return len(self.deg_files)\n    \n    def __getitem__(self, idx):\n        dimg = cv2.imread(self.deg_files[idx])\n        \n        if self.paired:\n            cimg = cv2.imread(self.cln_files[idx])  # Same index for paired\n        else:\n            cimg = cv2.imread(random.choice(self.cln_files))  # Random for unpaired\n        \n        if self.tf:\n            # Apply same augmentation to both (paired consistency)\n            seed = np.random.randint(0, 999999)\n            random.seed(seed); np.random.seed(seed)\n            dimg = self.tf(image=dimg)['image']\n            random.seed(seed); np.random.seed(seed)\n            cimg = self.tf(image=cimg)['image']\n        \n        return to_tensor(dimg), to_tensor(cimg), os.path.basename(self.deg_files[idx])\n\ndef build_loader_for(domain, batch_size=2, train=True, paired=False):\n    \"\"\"\n    Build dataloader for single domain\n    \n    Args:\n        domain: 'fog', 'rain', or 'lowlight'\n        batch_size: batch size\n        train: training or validation mode\n        paired: if True, use paired data (same index); if False, random clean images\n    \"\"\"\n    deg_dir = DATA_ROOTS[domain]\n    deg_files = list_images(deg_dir)\n    cln_files = list_images(CLEAN_DIR)\n    \n    tf = train_tf if train else val_tf\n    ds = PairedDataset(deg_files, cln_files, transform=tf, paired=paired)\n    \n    return DataLoader(\n        ds, \n        batch_size=batch_size, \n        shuffle=train, \n        num_workers=2, \n        pin_memory=True, \n        drop_last=train\n    ), len(deg_files)\n\n# Build loaders for all domains\ntrain_loaders = {}\nval_loaders   = {}\ncounts = {}\n\nfor d in DEG_DOMAINS:\n    # TRAIN: Unpaired (random clean images) - better for unlabeled scenario\n    tl, n = build_loader_for(d, batch_size=4, train=True, paired=False)\n    \n    # VAL: Use all samples, no pairing needed\n    vl, _ = build_loader_for(d, batch_size=1, train=False, paired=False)\n    \n    train_loaders[d] = tl\n    val_loaders[d]   = vl\n    counts[d] = n\n\nprint(\"‚úÖ Data loaders created (EDAR optimized)\")\nprint(\"\\nDomain image counts:\")\nfor d, n in counts.items():\n    print(f\"  {d:12s}: {n}\")\n\nprint(\"\\nTrain loader batches per domain:\")\nfor d, dl in train_loaders.items():\n    print(f\"  {d:12s}: {len(dl)} batches\")\n\nprint(\"\\nVal samples per domain:\")\nfor d, dl in val_loaders.items():\n    print(f\"  {d:12s}: {len(dl.dataset)} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T10:56:30.577889Z","iopub.execute_input":"2025-11-28T10:56:30.578544Z","iopub.status.idle":"2025-11-28T10:56:31.787841Z","shell.execute_reply.started":"2025-11-28T10:56:30.578519Z","shell.execute_reply":"2025-11-28T10:56:31.787143Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Data loaders created (EDAR optimized)\n\nDomain image counts:\n  fog         : 1000\n  rain        : 1000\n  lowlight    : 1000\n\nTrain loader batches per domain:\n  fog         : 250 batches\n  rain        : 250 batches\n  lowlight    : 250 batches\n\nVal samples per domain:\n  fog         : 1000 samples\n  rain        : 1000 samples\n  lowlight    : 1000 samples\n","output_type":"stream"}],"execution_count":91},{"cell_type":"code","source":"# ============================================================\n# CELL 13: EDAR MODULES (Edge-Aware Domain-Adaptive Restormer)\n# ============================================================\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# -------- 1. EDGE EXTRACTOR --------\nclass FastEdgeExtractor(nn.Module):\n    \"\"\"Sobel edge detection - fast and differentiable\"\"\"\n    def __init__(self):\n        super().__init__()\n        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32)\n        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32)\n        self.register_buffer('sobel_x', sobel_x.view(1, 1, 3, 3))\n        self.register_buffer('sobel_y', sobel_y.view(1, 1, 3, 3))\n        \n    def forward(self, x):\n        \"\"\"x: [B, 3, H, W] -> edges: [B, 1, H, W]\"\"\"\n        gray = 0.299*x[:,0:1] + 0.587*x[:,1:2] + 0.114*x[:,2:3]\n        grad_x = F.conv2d(gray, self.sobel_x, padding=1)\n        grad_y = F.conv2d(gray, self.sobel_y, padding=1)\n        edges = torch.sqrt(grad_x**2 + grad_y**2 + 1e-6)\n        return edges\n\n\n# -------- 2. DOMAIN ENCODER --------\nclass SimpleDomainEncoder(nn.Module):\n    \"\"\"Maps domain name to learnable embedding\"\"\"\n    def __init__(self, n_domains=3, embed_dim=64):\n        super().__init__()\n        self.embedding = nn.Embedding(n_domains, embed_dim)\n        self.domain_map = {'fog': 0, 'rain': 1, 'lowlight': 2}\n        \n    def forward(self, domain_name):\n        \"\"\"domain_name: str -> [1, embed_dim, 1, 1]\"\"\"\n        idx = torch.tensor([self.domain_map[domain_name]], device=self.embedding.weight.device)\n        emb = self.embedding(idx)\n        return emb[:, :, None, None]\n\n\n# -------- 3. EDGE-GUIDED REFINEMENT --------\nclass LightweightEdgeRefinement(nn.Module):\n    \"\"\"Injects edge info into features\"\"\"\n    def __init__(self, in_channels=96):\n        super().__init__()\n        self.edge_process = nn.Sequential(\n            nn.Conv2d(1, 16, 3, 1, 1, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(16, 32, 3, 1, 1, bias=False),\n            nn.ReLU(inplace=True)\n        )\n        self.gate = nn.Sequential(\n            nn.Conv2d(in_channels + 32, 32, 1, bias=False),\n            nn.Sigmoid()\n        )\n        self.fuse = nn.Conv2d(in_channels + 32, in_channels, 1, bias=False)\n        \n    def forward(self, feat, edge_map):\n        \"\"\"feat: [B, C, H, W], edge_map: [B, 1, H, W]\"\"\"\n        if edge_map.shape[2:] != feat.shape[2:]:\n            edge_map = F.interpolate(edge_map, size=feat.shape[2:], mode='bilinear', align_corners=False)\n        \n        edge_feat = self.edge_process(edge_map)\n        combined = torch.cat([feat, edge_feat], dim=1)\n        attn = self.gate(combined)\n        edge_weighted = edge_feat * attn\n        fused = self.fuse(torch.cat([feat, edge_weighted], dim=1))\n        return feat + fused\n\n\n# -------- 4. DOMAIN-ADAPTIVE MODULATION --------\nclass DomainModulation(nn.Module):\n    \"\"\"FiLM-style domain conditioning\"\"\"\n    def __init__(self, feat_channels=96, domain_dim=64):\n        super().__init__()\n        self.scale = nn.Linear(domain_dim, feat_channels)\n        self.shift = nn.Linear(domain_dim, feat_channels)\n        \n    def forward(self, feat, domain_emb):\n        \"\"\"feat: [B, C, H, W], domain_emb: [B, D, 1, 1]\"\"\"\n        domain_vec = domain_emb.squeeze(-1).squeeze(-1)\n        gamma = self.scale(domain_vec)[:, :, None, None]\n        beta = self.shift(domain_vec)[:, :, None, None]\n        return feat * (1 + gamma) + beta\n\n\n# -------- 5. EDAR WRAPPER --------\nclass EDARWrapper(nn.Module):\n    \"\"\"Wraps Restormer with EDAR components\"\"\"\n    def __init__(self, base_restormer, n_domains=3):\n        super().__init__()\n        self.restormer = base_restormer\n        \n        # EDAR modules\n        self.edge_extractor = FastEdgeExtractor()\n        self.domain_encoder = SimpleDomainEncoder(n_domains, embed_dim=64)\n        self.edge_refine = LightweightEdgeRefinement(in_channels=96)\n        self.domain_mod = DomainModulation(feat_channels=96, domain_dim=64)\n        \n        self.use_edar = True\n        \n    def forward(self, x, domain=None):\n        \"\"\"\n        x: [B, 3, H, W], domain: str or None\n        Returns: (output, edges)\n        \"\"\"\n        edges = self.edge_extractor(x)\n        \n        domain_emb = None\n        if domain is not None and self.use_edar:\n            domain_emb = self.domain_encoder(domain)\n        \n        # Restormer encoder\n        inp_enc = self.restormer.patch_embed(x)\n        enc1 = self.restormer.encoder_level1(inp_enc)\n        enc2 = self.restormer.encoder_level2(self.restormer.down1_2(enc1))\n        enc3 = self.restormer.encoder_level3(self.restormer.down2_3(enc2))\n        \n        # Bottleneck\n        latent = self.restormer.latent(self.restormer.down3_4(enc3))\n        \n        # Decoder\n        dec3 = self.restormer.decoder_level3(\n            self.restormer.reduce_chan_level3(\n                torch.cat([self.restormer.up4_3(latent), enc3], 1)\n            )\n        )\n        dec2 = self.restormer.decoder_level2(\n            self.restormer.reduce_chan_level2(\n                torch.cat([self.restormer.up3_2(dec3), enc2], 1)\n            )\n        )\n        dec1 = self.restormer.decoder_level1(\n            torch.cat([self.restormer.up2_1(dec2), enc1], 1)\n        )\n        \n        # EDAR injection\n        if domain_emb is not None:\n            dec1 = self.domain_mod(dec1, domain_emb)\n        \n        refined = self.restormer.refinement(dec1)\n        if self.use_edar:\n            refined = self.edge_refine(refined, edges)\n        \n        output = self.restormer.output(refined)\n        return output, edges\n    \n    def baseline_mode(self):\n        self.use_edar = False\n        \n    def edar_mode(self):\n        self.use_edar = True\n\nprint(\"‚úÖ EDAR modules loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T10:56:34.515093Z","iopub.execute_input":"2025-11-28T10:56:34.515793Z","iopub.status.idle":"2025-11-28T10:56:34.534261Z","shell.execute_reply.started":"2025-11-28T10:56:34.515768Z","shell.execute_reply":"2025-11-28T10:56:34.533410Z"}},"outputs":[{"name":"stdout","text":"‚úÖ EDAR modules loaded successfully!\n","output_type":"stream"}],"execution_count":92},{"cell_type":"code","source":"# ============================================================\n# CELL 14: TEST EDAR WRAPPER\n# ============================================================\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Wrap your existing model\nedar_model = EDARWrapper(model, n_domains=3).to(device)\nprint(f\"‚úÖ EDAR model created on {device}\")\n\n# Test forward pass\ntest_input = torch.randn(1, 3, 256, 256).to(device)\nwith torch.no_grad():\n    test_out, test_edges = edar_model(test_input, domain='fog')\n    \nprint(f\"‚úÖ Forward pass successful!\")\nprint(f\"   Input shape:  {tuple(test_input.shape)}\")\nprint(f\"   Output shape: {tuple(test_out.shape)}\")\nprint(f\"   Edges shape:  {tuple(test_edges.shape)}\")\n\n# Count trainable parameters\nedar_params = sum(p.numel() for p in edar_model.parameters() if p.requires_grad)\nprint(f\"‚úÖ Trainable parameters: {edar_params:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T10:56:37.942141Z","iopub.execute_input":"2025-11-28T10:56:37.942918Z","iopub.status.idle":"2025-11-28T10:56:38.034521Z","shell.execute_reply.started":"2025-11-28T10:56:37.942892Z","shell.execute_reply":"2025-11-28T10:56:38.033724Z"}},"outputs":[{"name":"stdout","text":"‚úÖ EDAR model created on cuda\n‚úÖ Forward pass successful!\n   Input shape:  (1, 3, 256, 256)\n   Output shape: (1, 3, 256, 256)\n   Edges shape:  (1, 1, 256, 256)\n‚úÖ Trainable parameters: 505,292\n","output_type":"stream"}],"execution_count":93},{"cell_type":"code","source":"CHECKPOINT_PATH = \"/kaggle/input/novelty-epoch-19/edar_multidomain_ep19.pth\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T10:56:40.825590Z","iopub.execute_input":"2025-11-28T10:56:40.826435Z","iopub.status.idle":"2025-11-28T10:56:40.830097Z","shell.execute_reply.started":"2025-11-28T10:56:40.826353Z","shell.execute_reply":"2025-11-28T10:56:40.829192Z"}},"outputs":[],"execution_count":94},{"cell_type":"code","source":"# ============================================================\n# CELL: Set CHECKPOINT_PATH here, smart-load checkpoint once, then upload image & run inference\n# Paste this AFTER your EDAR model class + `edar_model` are defined.\n# ============================================================\nimport os, re, torch, numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output\n\n# ------------- CONFIG: set your epoch-19 .pth path here -------------\n# Example Kaggle dataset path:\n# CHECKPOINT_PATH = \"/kaggle/input/novelty-epoch-19/edar_multidomain_ep19.pth\"\n# Or local working path if you uploaded to notebook files:\n# CHECKPOINT_PATH = \"/kaggle/working/edar_multidomain_ep19.pth\"\ntry:\n    CHECKPOINT_PATH  # if already set earlier, keep it\nexcept NameError:\n    CHECKPOINT_PATH = \"/kaggle/input/novelty-epoch-19/edar_multidomain_ep19.pth\"  # <-- update this if different\n\n# ------------- device fallback -------------\ntry:\n    device\nexcept NameError:\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprint(f\"Device: {device}\")\nprint(f\"Checkpoint path (will try to load now): {CHECKPOINT_PATH}\")\n\n# --------- Lightweight image utilities (same as used earlier) ----------\nimport cv2\ndef load_image_local(path, max_size=1024):\n    img = cv2.imread(path)\n    if img is None:\n        raise FileNotFoundError(f\"Image not found or unreadable: {path}\")\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    h, w = img.shape[:2]\n    if max(h, w) > max_size:\n        scale = max_size / max(h, w)\n        img = cv2.resize(img, (int(w*scale), int(h*scale)), interpolation=cv2.INTER_LANCZOS4)\n    return img\n\ndef img_to_tensor_local(img):\n    img = img.astype(np.float32) / 255.0\n    t = torch.from_numpy(np.transpose(img, (2,0,1))).unsqueeze(0)\n    return t\n\ndef tensor_to_img_local(t):\n    img = t.squeeze(0).permute(1,2,0).cpu().numpy()\n    img = np.clip(img * 255, 0, 255).astype(np.uint8)\n    return img\n\n# === Replace/patch restore_image_local with padding-handling version ===\nimport math, cv2, numpy as np, torch\nfrom PIL import Image\n\ndef pad_to_multiple(img_np, multiple=16):\n    h, w = img_np.shape[:2]\n    tgt_h = int(math.ceil(h / multiple) * multiple)\n    tgt_w = int(math.ceil(w / multiple) * multiple)\n    pad_h = tgt_h - h\n    pad_w = tgt_w - w\n    pad_top = pad_h // 2\n    pad_bottom = pad_h - pad_top\n    pad_left = pad_w // 2\n    pad_right = pad_w - pad_left\n    padded = np.pad(\n        img_np,\n        ((pad_top, pad_bottom), (pad_left, pad_right), (0, 0)),\n        mode='reflect'\n    )\n    return padded, (pad_top, pad_bottom, pad_left, pad_right)\n\ndef unpad_img(img_np, pads):\n    pad_top, pad_bottom, pad_left, pad_right = pads\n    h, w = img_np.shape[:2]\n    return img_np[pad_top:h-pad_bottom if pad_bottom>0 else h,\n                  pad_left:w-pad_right if pad_right>0 else w]\n\ndef img_to_tensor_with_padding(img_np, multiple=16, device='cuda'):\n    orig_h, orig_w = img_np.shape[:2]\n    padded_np, pads = pad_to_multiple(img_np, multiple=multiple)\n    tensor = torch.from_numpy(\n        np.transpose(padded_np.astype(np.float32)/255.0, (2,0,1))\n    ).unsqueeze(0).to(device)\n    return tensor, pads, (orig_h, orig_w)\n\ndef tensor_to_img_and_unpad(tensor, pads, orig_size):\n    img = tensor.squeeze(0).permute(1,2,0).cpu().numpy()\n    img = np.clip(img * 255, 0, 255).astype(np.uint8)\n    img = unpad_img(img, pads)\n    img = cv2.resize(img, (orig_size[1], orig_size[0]), interpolation=cv2.INTER_LINEAR)\n    return img\n\n@torch.no_grad()\ndef restore_image_local(model, image_path, domain='fog', device='cuda', pad_multiple=16):\n    img = cv2.imread(image_path)\n    if img is None:\n        raise FileNotFoundError(f\"Image not found or unreadable: {image_path}\")\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    inp_tensor, pads, orig_size = img_to_tensor_with_padding(img, multiple=pad_multiple, device=device)\n\n    model.to(device).eval()\n    out_t, edges_t = model(inp_tensor, domain=domain)\n\n    restored_img = tensor_to_img_and_unpad(out_t, pads, orig_size)\n\n    try:\n        if edges_t.dim() == 4 and edges_t.size(1) == 1:\n            edges_t3 = edges_t.repeat(1,3,1,1)\n        else:\n            edges_t3 = edges_t\n        edges_img = tensor_to_img_and_unpad(edges_t3, pads, orig_size)\n    except:\n        e = edges_t.squeeze().cpu().numpy()\n        if e.ndim == 2:\n            e = np.stack([e]*3, axis=-1)\n        e = (np.clip(e,0,1)*255).astype(np.uint8)\n        edges_img = unpad_img(e, pads)\n\n    return img, restored_img, edges_img\n\n\n# ---------- checkpoint helpers (smart .body handling) ----------\ndef pick_state_dict(ckpt):\n    for k in (\"edar_model\",\"state_dict\",\"model\"):\n        if isinstance(ckpt, dict) and k in ckpt:\n            return ckpt[k]\n    return ckpt\n\ndef strip_module(sd):\n    return { (k[7:] if k.startswith(\"module.\") else k): v for k,v in sd.items() }\n\ndef remove_body_dot(sd):\n    new = {}\n    for k,v in sd.items():\n        nk = k.replace(\".body.\", \".\")\n        if nk.endswith(\".body\"):\n            nk = nk[:-5]\n        new[nk] = v\n    return new\n\ndef add_body_dot_for_norms(sd):\n    # insert \".body.\" between \".normN\" and \".weight/.bias\" if missing\n    new = {}\n    for k,v in sd.items():\n        nk = re.sub(r\"(\\.norm\\d+)(\\.(?:weight|bias|gamma|beta))$\", r\"\\1.body\\2\", k)\n        new[nk] = v\n    return new\n\ndef try_best_load(model, ckpt_path, map_location=None):\n    \"\"\"\n    Try both removal and insertion of '.body.' norm naming and pick best by minimal mismatch.\n    Loads chosen version into `model` and returns (chosen_label, info).\n    \"\"\"\n    map_location = map_location or device\n    if not os.path.exists(ckpt_path):\n        raise FileNotFoundError(f\"Checkpoint path not found: {ckpt_path}\")\n    raw = torch.load(ckpt_path, map_location=map_location)\n    sd_raw = pick_state_dict(raw)\n    sd_strip = strip_module(sd_raw)\n\n    # Save original model weights (so we can test both variants without corruption)\n    orig_state = {k: v.clone().cpu() for k, v in model.state_dict().items()}\n\n    # Variant A: remove .body.\n    sd_a = remove_body_dot(sd_strip)\n    info_a = model.load_state_dict(sd_a, strict=False)\n    score_a = len(info_a.missing_keys) + len(info_a.unexpected_keys)\n\n    # restore original\n    model.load_state_dict(orig_state, strict=False)\n\n    # Variant B: add .body. for norm entries\n    sd_b = add_body_dot_for_norms(sd_strip)\n    info_b = model.load_state_dict(sd_b, strict=False)\n    score_b = len(info_b.missing_keys) + len(info_b.unexpected_keys)\n\n    # choose best\n    if score_a <= score_b:\n        model.load_state_dict(sd_a, strict=False)\n        chosen = \"remove .body. (A)\"\n        info = info_a\n    else:\n        model.load_state_dict(sd_b, strict=False)\n        chosen = \"add .body. for norms (B)\"\n        info = info_b\n\n    return chosen, info\n\n# ----------------- Try to load checkpoint NOW (once) -----------------\nloaded_info = None\nloaded_strategy = None\ntry:\n    chosen, info = try_best_load(edar_model, CHECKPOINT_PATH, map_location=device)\n    loaded_strategy, loaded_info = chosen, info\n    print(\"Checkpoint load strategy:\", chosen)\n    print(f\"Missing keys: {len(info.missing_keys)} | Unexpected keys: {len(info.unexpected_keys)}\")\n    if info.missing_keys:\n        print(\"  Example missing keys (up to 12):\")\n        for k in info.missing_keys[:12]:\n            print(\"   \", k)\n    if info.unexpected_keys:\n        print(\"  Example unexpected keys (up to 12):\")\n        for k in info.unexpected_keys[:12]:\n            print(\"   \", k)\nexcept Exception as e:\n    print(\"‚ùå Checkpoint load failed:\", e)\n    import traceback; traceback.print_exc()\n\n# ----------------- UI: upload image + run inference -----------------\nimg_upload = widgets.FileUpload(accept='image/*', multiple=False, description='Upload Image')\ndomain_selector = widgets.Dropdown(options=['fog','rain','lowlight'], value='fog', description='Domain:')\nrun_btn = widgets.Button(description='Run EDAR', button_style='success')\nout = widgets.Output(layout={'border':'1px solid black'})\n\ndef save_uploaded_image(upload_widget, target_prefix='uploaded_input'):\n    val = upload_widget.value\n    if not val:\n        return None\n    # dict-style\n    if isinstance(val, dict):\n        fname = list(val.keys())[0]\n        entry = val[fname]\n        content = entry.get('content') if isinstance(entry, dict) else entry\n        ext = os.path.splitext(fname)[1] or '.png'\n        out_path = f'./{target_prefix}{ext}'\n        with open(out_path, 'wb') as f:\n            f.write(content)\n        return out_path\n    # list/tuple style\n    if isinstance(val, (list, tuple)):\n        entry = val[0]\n        if isinstance(entry, dict) and 'name' in entry and 'content' in entry:\n            fname = entry['name']; content = entry['content']\n        elif isinstance(entry, (list, tuple)) and len(entry) >= 2:\n            fname = entry[0]; content = entry[1]\n        else:\n            fname = getattr(entry, 'name', f'{target_prefix}.png'); content = entry if isinstance(entry, (bytes, bytearray)) else None\n        if content is None:\n            raise ValueError(\"Couldn't extract uploaded image bytes.\")\n        ext = os.path.splitext(fname)[1] or '.png'\n        out_path = f'./{target_prefix}{ext}'\n        with open(out_path, 'wb') as f:\n            f.write(content)\n        return out_path\n    # raw bytes\n    if isinstance(val, (bytes, bytearray)):\n        out_path = f'./{target_prefix}.png'\n        with open(out_path, 'wb') as f:\n            f.write(val)\n        return out_path\n    raise ValueError(\"Unrecognized upload.value format: \" + str(type(val)))\n\ndef on_run(b):\n    with out:\n        clear_output()\n        # ensure checkpoint was loaded\n        if loaded_info is None:\n            print(\"‚ùå Checkpoint was not successfully loaded earlier. Fix CHECKPOINT_PATH and re-run this cell.\")\n            return\n\n        # save uploaded image\n        if not img_upload.value:\n            print(\"‚ùå Please upload an input image.\")\n            return\n        try:\n            img_path = save_uploaded_image(img_upload)\n        except Exception as e:\n            print(\"‚ùå Failed to save uploaded image:\", e)\n            import traceback; traceback.print_exc()\n            return\n\n        print(\"üì• Uploaded image saved to:\", img_path)\n        print(\"‚ñ∂ Running inference (domain =\", domain_selector.value, \") ...\")\n\n        try:\n            edar_model.to(device).eval()\n            inp, restored, edges = restore_image_local(edar_model, img_path, domain=domain_selector.value, device=device)\n        except Exception:\n            import traceback; traceback.print_exc()\n            print(\"‚ùå Inference failed.\")\n            return\n\n        # show results\n        try:\n            fig, ax = plt.subplots(1,3, figsize=(18,6))\n            ax[0].imshow(inp); ax[0].axis('off'); ax[0].set_title(f\"Input ({domain_selector.value})\")\n            ax[1].imshow(restored); ax[1].axis('off'); ax[1].set_title(\"Restored\")\n            ax[2].imshow(edges); ax[2].axis('off'); ax[2].set_title(\"Edge Map\")\n            plt.tight_layout()\n            plt.show()\n            Image.fromarray(restored).save(\"restored_output.png\")\n            fig.savefig(\"comparison_output.png\", dpi=150, bbox_inches='tight')\n            print(\"‚úÖ Saved: restored_output.png, comparison_output.png\")\n        except Exception:\n            import traceback; traceback.print_exc()\n            print(\"‚ùå Failed to display/save outputs.\")\n\nrun_btn.on_click(on_run)\n\ndisplay(widgets.VBox([\n    widgets.HTML(\"<h3>EDAR Tester ‚Äî checkpoint auto-loaded; upload input image to run</h3>\"),\n    widgets.HBox([img_upload, domain_selector, run_btn]),\n    out\n]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T11:01:48.782179Z","iopub.execute_input":"2025-11-28T11:01:48.782744Z","iopub.status.idle":"2025-11-28T11:01:49.416685Z","shell.execute_reply.started":"2025-11-28T11:01:48.782719Z","shell.execute_reply":"2025-11-28T11:01:49.415865Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nCheckpoint path (will try to load now): /kaggle/input/novelty-epoch-19/edar_multidomain_ep19.pth\nCheckpoint load strategy: add .body. for norms (B)\nMissing keys: 0 | Unexpected keys: 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<h3>EDAR Tester ‚Äî checkpoint auto-loaded; upload input image to run</h3>'), HBox(ch‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d871df5f11d94347ad25e18bf25da455"}},"metadata":{}}],"execution_count":101},{"cell_type":"code","source":"# ------------------------\n# Gradio UI for EDAR (no input shown in outputs)\n# Paste this after EDAR class definitions and after edar_model is created.\n# Edit CHECKPOINT_PATH below to point to your epoch-19 .pth file.\n# ------------------------\nimport os, re, math, io, torch, numpy as np\nfrom PIL import Image\nimport cv2\nimport gradio as gr\n\n# ---------------- CONFIG: set your checkpoint path here ----------------\ntry:\n    CHECKPOINT_PATH  # keep existing if already set\nexcept NameError:\n    CHECKPOINT_PATH = \"/kaggle/input/novelty-epoch-19/edar_multidomain_ep19.pth\"\n\n# ---------------- device ----------------\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\nprint(\"Checkpoint:\", CHECKPOINT_PATH)\n\n# ----------------- sanity check for edar_model -----------------\ntry:\n    edar_model\nexcept NameError:\n    raise RuntimeError(\"`edar_model` is not defined. Create EDARWrapper(base_restormer) and assign to edar_model before running this cell.\")\n\n# ----------------- Utilities: padding + tensor conversions -----------------\ndef pad_to_multiple(img_np, multiple=16):\n    h, w = img_np.shape[:2]\n    tgt_h = int(math.ceil(h / multiple) * multiple)\n    tgt_w = int(math.ceil(w / multiple) * multiple)\n    pad_h = tgt_h - h\n    pad_w = tgt_w - w\n    pad_top = pad_h // 2\n    pad_bottom = pad_h - pad_top\n    pad_left = pad_w // 2\n    pad_right = pad_w - pad_left\n    padded = np.pad(img_np, ((pad_top, pad_bottom), (pad_left, pad_right), (0, 0)), mode='reflect')\n    return padded, (pad_top, pad_bottom, pad_left, pad_right)\n\ndef unpad_img(img_np, pads):\n    pad_top, pad_bottom, pad_left, pad_right = pads\n    h, w = img_np.shape[:2]\n    top = pad_top\n    bottom = h - pad_bottom if pad_bottom>0 else h\n    left = pad_left\n    right = w - pad_right if pad_right>0 else w\n    return img_np[top:bottom, left:right]\n\ndef pil_to_rgb_np(pil_img):\n    arr = np.array(pil_img.convert(\"RGB\"))\n    return arr\n\ndef img_to_tensor_with_padding(img_np, multiple=16, device='cuda'):\n    orig_h, orig_w = img_np.shape[:2]\n    padded_np, pads = pad_to_multiple(img_np, multiple=multiple)\n    tensor = torch.from_numpy(np.transpose(padded_np.astype(np.float32)/255.0, (2,0,1))).unsqueeze(0).to(device)\n    return tensor, pads, (orig_h, orig_w)\n\ndef tensor_to_img_and_unpad(tensor, pads, orig_size):\n    img = tensor.squeeze(0).permute(1,2,0).cpu().numpy()\n    img = np.clip(img * 255, 0, 255).astype(np.uint8)\n    img = unpad_img(img, pads)\n    img = cv2.resize(img, (orig_size[1], orig_size[0]), interpolation=cv2.INTER_LINEAR)\n    return img\n\n# ----------------- smart checkpoint loader (try both variants) -----------------\ndef pick_state_dict(ckpt):\n    for k in (\"edar_model\",\"state_dict\",\"model\"):\n        if isinstance(ckpt, dict) and k in ckpt:\n            return ckpt[k]\n    return ckpt\n\ndef strip_module(sd):\n    return { (k[7:] if k.startswith(\"module.\") else k): v for k,v in sd.items() }\n\ndef remove_body_dot(sd):\n    new = {}\n    for k,v in sd.items():\n        nk = k.replace(\".body.\", \".\")\n        if nk.endswith(\".body\"):\n            nk = nk[:-5]\n        new[nk] = v\n    return new\n\ndef add_body_dot_for_norms(sd):\n    new = {}\n    for k,v in sd.items():\n        nk = re.sub(r\"(\\.norm\\d+)(\\.(?:weight|bias|gamma|beta))$\", r\"\\1.body\\2\", k)\n        new[nk] = v\n    return new\n\ndef try_best_load(model, ckpt_path, map_location=None):\n    map_location = map_location or device\n    if not os.path.exists(ckpt_path):\n        raise FileNotFoundError(f\"Checkpoint not found: {ckpt_path}\")\n    raw = torch.load(ckpt_path, map_location=map_location)\n    sd_raw = pick_state_dict(raw)\n    sd_strip = strip_module(sd_raw)\n\n    # save original model state (cpu copy)\n    orig_state = {k: v.clone().cpu() for k,v in model.state_dict().items()}\n\n    # Variant A\n    sd_a = remove_body_dot(sd_strip)\n    info_a = model.load_state_dict(sd_a, strict=False)\n    model.load_state_dict(orig_state, strict=False)\n\n    # Variant B\n    sd_b = add_body_dot_for_norms(sd_strip)\n    info_b = model.load_state_dict(sd_b, strict=False)\n    # choose best by fewer mismatches\n    score_a = len(info_a.missing_keys) + len(info_a.unexpected_keys)\n    score_b = len(info_b.missing_keys) + len(info_b.unexpected_keys)\n    if score_a <= score_b:\n        model.load_state_dict(sd_a, strict=False)\n        chosen, info = \"remove .body. (A)\", info_a\n    else:\n        model.load_state_dict(sd_b, strict=False)\n        chosen, info = \"add .body. for norms (B)\", info_b\n    return chosen, info\n\n# ----------------- Load checkpoint ONCE -----------------\nprint(\"Loading checkpoint (smart)...\")\ntry:\n    chosen, info = try_best_load(edar_model, CHECKPOINT_PATH, map_location=device)\n    print(\"Loaded checkpoint with strategy:\", chosen)\n    print(f\"Missing keys: {len(info.missing_keys)} | Unexpected keys: {len(info.unexpected_keys)}\")\nexcept Exception as e:\n    raise RuntimeError(f\"Failed to load checkpoint: {e}\")\n\n# ----------------- Inference wrapper used by Gradio (returns only restored + edges) -----------------\n@torch.no_grad()\ndef gradio_infer(pil_img, domain=\"fog\"):\n    if pil_img is None:\n        return None, None\n    inp_np = pil_to_rgb_np(pil_img)\n    inp_tensor, pads, orig_size = img_to_tensor_with_padding(inp_np, multiple=16, device=device)\n    edar_model.to(device).eval()\n    out_t, edges_t = edar_model(inp_tensor, domain=domain)\n    restored_np = tensor_to_img_and_unpad(out_t, pads, orig_size)\n    if edges_t.dim() == 4 and edges_t.size(1) == 1:\n        edges_t3 = edges_t.repeat(1,3,1,1)\n    else:\n        edges_t3 = edges_t\n    edges_np = tensor_to_img_and_unpad(edges_t3, pads, orig_size)\n    restored_pil = Image.fromarray(restored_np)\n    edges_pil = Image.fromarray(edges_np)\n    return restored_pil, edges_pil\n\n# ----------------- Build Gradio Interface (no input output) -----------------\ndemo = gr.Interface(\n    fn=gradio_infer,\n    inputs=[\n        gr.Image(label=\"Upload Input Image\", type=\"pil\"),\n        gr.Dropdown(choices=[\"fog\",\"rain\",\"lowlight\"], value=\"fog\", label=\"Domain\")\n    ],\n    outputs=[\n        gr.Image(type=\"pil\", label=\"EDAR Restored\"),\n        gr.Image(type=\"pil\", label=\"Edge Map\")\n    ],\n    title=\"EDAR ‚Äî Image Restoration (multi-domain)\",\n    description=\"Upload an image, pick domain (fog/rain/lowlight). The model is loaded from CHECKPOINT_PATH set in the cell.\",\n    allow_flagging=\"never\",\n    examples=None\n)\n\n# ----------------- Launch Gradio -----------------\nprint(\"Launching Gradio app... (press stop in the notebook to kill)\")\ndemo.launch(share=True, server_name=\"0.0.0.0\", server_port=7868)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T11:21:08.752869Z","iopub.execute_input":"2025-11-28T11:21:08.753539Z","iopub.status.idle":"2025-11-28T11:21:09.767899Z","shell.execute_reply.started":"2025-11-28T11:21:08.753510Z","shell.execute_reply":"2025-11-28T11:21:09.767187Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nCheckpoint: /kaggle/input/novelty-epoch-19/edar_multidomain_ep19.pth\nLoading checkpoint (smart)...\nLoaded checkpoint with strategy: add .body. for norms (B)\nMissing keys: 0 | Unexpected keys: 0\nLaunching Gradio app... (press stop in the notebook to kill)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/gradio/interface.py:425: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated. Use `flagging_mode` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://0.0.0.0:7868\n* Running on public URL: https://626d2a5197f8dfe158.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://626d2a5197f8dfe158.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":110,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":110},{"cell_type":"code","source":"# # ============================================================\n# # CELL 15 (UPDATED): TRAINING FUNCTION WITH RESUME SUPPORT\n# # ============================================================\n\n# from tqdm import tqdm\n# import os\n# import torch.nn as nn\n# import torchvision\n# from itertools import cycle\n# import matplotlib.pyplot as plt\n# from IPython.display import display\n# import numpy as np\n\n# def train_edar_multidomain_with_gan(\n#     edar_model,\n#     discriminator,\n#     vgg_model,\n#     train_loaders,\n#     val_loaders,\n#     epochs=11,\n#     device='cuda',\n#     save_dir='./edar_multidomain_checkpoints',\n#     start_epoch=9,  # NEW: Start from this epoch\n#     resume_checkpoint=\"/kaggle/input/novelty-epoch-8/edar_multidomain_ep08.pth\"  # NEW: Path to checkpoint to resume from\n# ):\n#     \"\"\"\n#     Train EDAR on all 3 domains with support for resuming\n    \n#     Args:\n#         start_epoch: Epoch number to start from (default: 1)\n#         resume_checkpoint: Path to checkpoint file to resume from (optional)\n#     \"\"\"\n    \n#     os.makedirs(save_dir, exist_ok=True)\n#     os.makedirs('./edar_multidomain_samples', exist_ok=True)\n#     for domain in ['fog', 'rain', 'lowlight']:\n#         os.makedirs(f'./edar_multidomain_samples/{domain}', exist_ok=True)\n    \n#     # Setup\n#     for param in edar_model.restormer.parameters():\n#         param.requires_grad = False\n#     for param in edar_model.restormer.refinement.parameters():\n#         param.requires_grad = True\n#     for param in edar_model.restormer.output.parameters():\n#         param.requires_grad = True\n    \n#     gen_params = (\n#         list(edar_model.edge_extractor.parameters()) +\n#         list(edar_model.domain_encoder.parameters()) +\n#         list(edar_model.edge_refine.parameters()) +\n#         list(edar_model.domain_mod.parameters()) +\n#         list(edar_model.restormer.refinement.parameters()) +\n#         list(edar_model.restormer.output.parameters())\n#     )\n    \n#     opt_G = torch.optim.Adam(gen_params, lr=1e-5, betas=(0.9, 0.999))\n#     opt_D = torch.optim.Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n    \n#     # ===== RESUME FROM CHECKPOINT =====\n#     if resume_checkpoint is not None and os.path.exists(resume_checkpoint):\n#         print(f\"üìÇ Loading checkpoint: {resume_checkpoint}\")\n#         checkpoint = torch.load(resume_checkpoint, map_location=device)\n        \n#         edar_model.load_state_dict(checkpoint['edar_model'])\n#         discriminator.load_state_dict(checkpoint['discriminator'])\n        \n#         # Load optimizer states if available\n#         if 'opt_G' in checkpoint:\n#             opt_G.load_state_dict(checkpoint['opt_G'])\n#             print(\"   ‚úÖ Loaded Generator optimizer state\")\n#         if 'opt_D' in checkpoint:\n#             opt_D.load_state_dict(checkpoint['opt_D'])\n#             print(\"   ‚úÖ Loaded Discriminator optimizer state\")\n        \n#         print(f\"   ‚úÖ Resumed from epoch {checkpoint.get('epoch', start_epoch-1)}\")\n#         print()\n    \n#     l1_loss = nn.L1Loss()\n#     mse_loss = nn.MSELoss()\n    \n#     def adv_loss(pred, is_real):\n#         target = torch.ones_like(pred) if is_real else torch.zeros_like(pred)\n#         return mse_loss(pred, target)\n    \n#     L_ADV, L_PERC, L_ID, L_EDGE = 1.0, 0.1, 0.1, 0.2\n    \n#     scaler_G = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n#     scaler_D = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n    \n#     # Round-robin iterator\n#     def round_robin_batches(loaders_dict):\n#         iters = {k: cycle(v) for k, v in loaders_dict.items()}\n#         domains = list(loaders_dict.keys())\n#         while True:\n#             for domain in domains:\n#                 yield domain, next(iters[domain])\n    \n#     steps_per_epoch = min(len(dl) for dl in train_loaders.values()) * len(train_loaders)\n#     rr_iter = round_robin_batches(train_loaders)\n    \n#     if start_epoch == 1:\n#         print(f\"Steps per epoch: {steps_per_epoch} (balanced across domains)\")\n#     else:\n#         print(f\"Resuming training: Epochs {start_epoch} ‚Üí {start_epoch + epochs - 1}\")\n#         print(f\"Steps per epoch: {steps_per_epoch}\")\n#     print()\n    \n#     # Training loop\n#     best_loss = float('inf')\n    \n#     # Calculate end epoch\n#     end_epoch = start_epoch + epochs\n    \n#     for epoch in range(start_epoch, end_epoch):\n#         edar_model.train()\n#         discriminator.train()\n        \n#         epoch_losses = {\n#             'G_total': [], 'G_adv': [], 'G_perc': [], \n#             'G_id': [], 'G_edge': [], 'D': [],\n#             'fog': [], 'rain': [], 'lowlight': []\n#         }\n        \n#         pbar = tqdm(range(steps_per_epoch), desc=f'Epoch {epoch}/{end_epoch-1}')\n        \n#         for step in pbar:\n#             domain, (degraded, clean, _) = next(rr_iter)\n#             degraded, clean = degraded.to(device), clean.to(device)\n            \n#             # ========== GENERATOR STEP ==========\n#             opt_G.zero_grad(set_to_none=True)\n            \n#             with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n#                 fake, edges_pred = edar_model(degraded, domain=domain)\n#                 edges_target = edar_model.edge_extractor(clean)\n                \n#                 loss_g_adv = adv_loss(discriminator(fake), True) * L_ADV\n#                 loss_g_perc = l1_loss(vgg_model(fake), vgg_model(clean)) * L_PERC\n                \n#                 identity, _ = edar_model(clean, domain=domain)\n#                 loss_g_id = l1_loss(identity, clean) * L_ID\n                \n#                 loss_g_edge = l1_loss(edges_pred, edges_target) * L_EDGE\n                \n#                 loss_G = loss_g_adv + loss_g_perc + loss_g_id + loss_g_edge\n            \n#             scaler_G.scale(loss_G).backward()\n#             scaler_G.step(opt_G)\n#             scaler_G.update()\n            \n#             # ========== DISCRIMINATOR STEP ==========\n#             opt_D.zero_grad(set_to_none=True)\n            \n#             with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n#                 pred_real = discriminator(clean)\n#                 pred_fake = discriminator(fake.detach())\n#                 loss_D = 0.5 * (adv_loss(pred_real, True) + adv_loss(pred_fake, False))\n            \n#             scaler_D.scale(loss_D).backward()\n#             scaler_D.step(opt_D)\n#             scaler_D.update()\n            \n#             # Track losses\n#             epoch_losses['G_total'].append(loss_G.item())\n#             epoch_losses['G_adv'].append(loss_g_adv.item())\n#             epoch_losses['G_perc'].append(loss_g_perc.item())\n#             epoch_losses['G_id'].append(loss_g_id.item())\n#             epoch_losses['G_edge'].append(loss_g_edge.item())\n#             epoch_losses['D'].append(loss_D.item())\n#             epoch_losses[domain].append(loss_G.item())\n            \n#             pbar.set_postfix({\n#                 'G': f'{loss_G.item():.3f}',\n#                 'D': f'{loss_D.item():.3f}',\n#                 'Edge': f'{loss_g_edge.item():.3f}',\n#                 'dom': domain\n#             })\n        \n#         # ========== EPOCH SUMMARY ==========\n#         print(f\"\\n{'='*60}\")\n#         print(f\"Epoch {epoch}/{end_epoch-1} Summary:\")\n#         print(f\"  Overall Generator Loss: {np.mean(epoch_losses['G_total']):.4f}\")\n#         print(f\"    ‚îú‚îÄ Adversarial: {np.mean(epoch_losses['G_adv']):.4f}\")\n#         print(f\"    ‚îú‚îÄ Perceptual:  {np.mean(epoch_losses['G_perc']):.4f}\")\n#         print(f\"    ‚îú‚îÄ Identity:    {np.mean(epoch_losses['G_id']):.4f}\")\n#         print(f\"    ‚îî‚îÄ Edge (NEW):  {np.mean(epoch_losses['G_edge']):.4f}\")\n#         print(f\"  Discriminator Loss:     {np.mean(epoch_losses['D']):.4f}\")\n#         print(f\"\\n  Per-Domain Generator Losses:\")\n#         print(f\"    üå´Ô∏è  Fog:      {np.mean(epoch_losses['fog']):.4f}\")\n#         print(f\"    üåßÔ∏è  Rain:     {np.mean(epoch_losses['rain']):.4f}\")\n#         print(f\"    üåô Lowlight: {np.mean(epoch_losses['lowlight']):.4f}\")\n#         print(f\"{'='*60}\\n\")\n        \n#         # ========== SAVE & DISPLAY SAMPLES ==========\n#         print(\"Generating preview samples...\")\n#         sample_images = save_and_collect_samples(\n#             edar_model, val_loaders, epoch, device\n#         )\n        \n#         # Display inline\n#         display_samples_inline(sample_images, epoch)\n        \n#         # ========== CHECKPOINTING ==========\n#         avg_G = np.mean(epoch_losses['G_total'])\n#         ckpt_path = f'{save_dir}/edar_multidomain_ep{epoch:02d}.pth'\n#         torch.save({\n#             'epoch': epoch,\n#             'edar_model': edar_model.state_dict(),\n#             'discriminator': discriminator.state_dict(),\n#             'opt_G': opt_G.state_dict(),\n#             'opt_D': opt_D.state_dict(),\n#         }, ckpt_path)\n        \n#         if avg_G < best_loss:\n#             best_loss = avg_G\n#             best_path = f'{save_dir}/edar_multidomain_best.pth'\n#             torch.save(edar_model.state_dict(), best_path)\n#             print(f\"‚úÖ Best model saved: {best_path}\\n\")\n    \n#     print(f\"\\n{'='*60}\")\n#     print(\"‚úÖ TRAINING COMPLETE!\")\n#     print(f\"   Trained epochs: {start_epoch} ‚Üí {end_epoch-1}\")\n#     print(f\"{'='*60}\")\n#     return edar_model\n\n\n# def save_and_collect_samples(model, val_loaders, epoch, device):\n#     \"\"\"Save samples for all domains and return them for display\"\"\"\n#     import torchvision.utils as vutils\n    \n#     model.eval()\n#     sample_images = {}\n    \n#     with torch.no_grad():\n#         for domain in ['fog', 'rain', 'lowlight']:\n#             domain_samples = []\n            \n#             for idx, (degraded, clean, name) in enumerate(val_loaders[domain]):\n#                 if idx >= 2:\n#                     break\n                \n#                 degraded = degraded.to(device)\n#                 clean = clean.to(device)\n                \n#                 restored, edges = model(degraded, domain=domain)\n#                 restored = torch.clamp(restored, 0, 1)\n                \n#                 grid = torch.cat([\n#                     degraded[0:1],\n#                     restored[0:1],\n#                     clean[0:1],\n#                     edges[0:1].repeat(1, 3, 1, 1)\n#                 ], dim=3)\n                \n#                 save_path = f'./edar_multidomain_samples/{domain}/ep{epoch:02d}_sample{idx}.png'\n#                 vutils.save_image(grid, save_path, normalize=False)\n                \n#                 grid_np = grid[0].permute(1, 2, 0).cpu().numpy()\n#                 domain_samples.append(grid_np)\n            \n#             sample_images[domain] = domain_samples\n    \n#     model.train()\n#     return sample_images\n\n\n# def display_samples_inline(sample_images, epoch):\n#     \"\"\"Display samples inline in notebook\"\"\"\n#     fig, axes = plt.subplots(3, 2, figsize=(20, 15))\n    \n#     domains = ['fog', 'rain', 'lowlight']\n#     domain_icons = {'fog': 'üå´Ô∏è', 'rain': 'üåßÔ∏è', 'lowlight': 'üåô'}\n    \n#     for row_idx, domain in enumerate(domains):\n#         samples = sample_images[domain]\n        \n#         for col_idx, sample in enumerate(samples):\n#             ax = axes[row_idx, col_idx]\n#             ax.imshow(sample)\n#             ax.axis('off')\n            \n#             if col_idx == 0:\n#                 ax.set_title(\n#                     f\"{domain_icons[domain]} {domain.upper()} - Sample {col_idx+1}\\n\"\n#                     f\"[Degraded | Restored | Clean | Edges]\",\n#                     fontsize=12, fontweight='bold', pad=10\n#                 )\n#             else:\n#                 ax.set_title(\n#                     f\"Sample {col_idx+1}\\n[Degraded | Restored | Clean | Edges]\",\n#                     fontsize=12, fontweight='bold', pad=10\n#                 )\n    \n#     plt.suptitle(\n#         f'EPOCH {epoch} - EDAR Multi-Domain Results',\n#         fontsize=16, fontweight='bold', y=0.995\n#     )\n#     plt.tight_layout()\n#     plt.show()\n    \n#     print(f\"‚úÖ Samples saved to: ./edar_multidomain_samples/[domain]/ep{epoch:02d}_sampleX.png\\n\")\n\n\n# print(\"‚úÖ Updated training function with resume support ready!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T10:55:46.739682Z","iopub.execute_input":"2025-11-28T10:55:46.739967Z","iopub.status.idle":"2025-11-28T10:55:46.750257Z","shell.execute_reply.started":"2025-11-28T10:55:46.739943Z","shell.execute_reply":"2025-11-28T10:55:46.749678Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":79},{"cell_type":"code","source":"# # ============================================================\n# # CELL 16: SETUP DISCRIMINATOR + VGG (Your Working Setup)\n# # ============================================================\n\n# import torch.nn as nn\n# import torchvision\n\n# # ===== DISCRIMINATOR (Your PatchGAN) =====\n# class PatchD(nn.Module):\n#     def __init__(self, in_ch=3, base=64):\n#         super().__init__()\n#         def blk(ic, oc, norm=True):\n#             m=[nn.Conv2d(ic, oc, 4, 2, 1)]\n#             if norm: m+=[nn.InstanceNorm2d(oc, affine=True)]\n#             m+=[nn.LeakyReLU(0.2, inplace=True)]\n#             return nn.Sequential(*m)\n#         self.net = nn.Sequential(\n#             blk(in_ch, base, norm=False),\n#             blk(base, base*2),\n#             blk(base*2, base*4),\n#             nn.Conv2d(base*4, 1, 3, 1, 1)\n#         )\n#     def forward(self, x): return self.net(x)\n\n# disc = PatchD().to(device).train()\n# print(\"‚úÖ Discriminator loaded\")\n\n# # ===== VGG FOR PERCEPTUAL LOSS =====\n# vgg = torchvision.models.vgg19(\n#     weights=torchvision.models.VGG19_Weights.IMAGENET1K_V1\n# ).features[:16].eval().to(device)\n\n# for p in vgg.parameters(): \n#     p.requires_grad = False\n\n# print(\"‚úÖ VGG-19 loaded for perceptual loss\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T10:55:46.750927Z","iopub.execute_input":"2025-11-28T10:55:46.751083Z","iopub.status.idle":"2025-11-28T10:55:46.772833Z","shell.execute_reply.started":"2025-11-28T10:55:46.751070Z","shell.execute_reply":"2025-11-28T10:55:46.772167Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"# # ============================================================\n# # CELL 17: TRAIN MULTI-DOMAIN EDAR\n# # ============================================================\n\n# print(\"=\"*60)\n# print(\"üåê TRAINING EDAR ACROSS ALL DOMAINS (FOG + RAIN + LOWLIGHT)\")\n# print(\"=\"*60)\n\n# # Wrap model\n# if 'edar_model' not in locals():\n#     edar_model = EDARWrapper(model, n_domains=3).to(device)\n\n# # Train on ALL domains\n# edar_multidomain = train_edar_multidomain_with_gan(\n#     edar_model=edar_model,\n#     discriminator=disc,\n#     vgg_model=vgg,\n#     train_loaders=train_loaders,  # Pass all 3 loaders!\n#     val_loaders=val_loaders,\n#     epochs=11,\n#     device=device,\n#     save_dir='./edar_multidomain_checkpoints'\n# )\n\n# print(\"\\n‚úÖ MULTI-DOMAIN TRAINING COMPLETE!\")\n# print(\"Model can now handle fog, rain, AND lowlight with domain adaptation!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T10:55:46.773513Z","iopub.execute_input":"2025-11-28T10:55:46.774007Z","iopub.status.idle":"2025-11-28T10:55:46.790052Z","shell.execute_reply.started":"2025-11-28T10:55:46.773990Z","shell.execute_reply":"2025-11-28T10:55:46.789402Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":81}]}